# Level 8 Diagnostic: Moral Judgment Under Cognitive Load

---

## The Falsifiable Claim

**The deepest structural problem: This study embeds dual-process theory simultaneously as its theoretical framework, its independent variable operationalization, its dependent variable classification, and its stimulus selection criterion — making it structurally incapable of falsifying the theory it claims to test.**

Specifically: "personal" dilemmas are defined by the theory as emotionally-driven; cognitive load is operationalized as impairing System 2 (the emotion-inhibitor); utilitarian responses are coded as System 1 outputs. The prediction that load increases utilitarian responses to personal dilemmas isn't testing dual-process theory — it's *replaying* it.

---

## Three Experts in Conflict

### Expert 1 (Defender)
The circularity charge is unwarranted. Using validated stimuli from the literature *is* the appropriate methodology — it's what cumulative science looks like. The personal/impersonal distinction has convergent validity from fMRI, response time, and emotional experience data collected independently of this paradigm. More critically: the study *can* be falsified. If high load + time pressure *doesn't* increase utilitarian responses to personal dilemmas, that's a genuine challenge to the dual-process account. The manipulations are independent of the classification.

### Expert 2 (Attacker)
The problem isn't circularity — it's construct validity of the manipulation. A 7-digit memorization task impairs phonological working memory. The theory requires impairment of *deliberative reasoning that inhibits emotional responses*. These are not the same cognitive resource. The study contains no manipulation check confirming that cognitive load impairs the *specific* capacity needed for deontological judgment. Without that, the null result and the positive result are equally uninterpretable. The design has an unfalsifiable interior even if its exterior looks testable.

### Expert 3 (Prober — What Both Take for Granted)
Both experts assume the 7-point Likert scale validly tracks a utilitarian-deontological dimension. But Kahane et al. (2018) found that "utilitarian" trolley responses do not correlate with actual utilitarian philosophy endorsement. The scale may measure *comfort with harm* or *willingness to appear decisive*, not moral reasoning architecture. Both experts are debating the experimental structure while sharing a faith that the output variable means what the researchers say it means.

---

## Transformation of the Claim

| Stage | Claim |
|-------|-------|
| **Original** | The design is circular: theory is embedded in both manipulations and measurements |
| **After Defender** | The circularity is defensible if manipulations are independent of classifications |
| **After Attacker** | The real problem is mechanism agnosticism: we don't know *which* cognitive resource is being manipulated |
| **After Prober** | **Transformed**: The entire design presupposes the psychological reality of a dual-process moral architecture that may not exist as a cognitive mechanism — and the measurement instrument (the Likert scale) cannot distinguish between that architecture and several competing accounts |

**The gap between original and transformed claim:**  
*Circularity* (logical structure problem) → *Construct invalidity* (measurement problem) → *Theoretical presupposition realism* (the problem is that the study treats contested theoretical entities as natural kinds).

The gap itself is diagnostic: the original claim was about the experiment's logic. The transformed claim reveals the experiment cannot determine whether its categories correspond to anything real.

---

## The Concealment Mechanism

**Methodological Respectability Laundering via Paradigm Citation**

Each design element is individually unimpeachable:
- IRB approved ✓
- Randomization ✓  
- Power analysis ✓
- Bonferroni correction ✓
- Established stimuli ✓
- Repeated measures ✓

The phrase "established stimuli" is doing the critical concealing work. It converts the act of *importing a contested theoretical classification* into the appearance of *methodological conservatism*. The personal/impersonal distinction isn't presented as a theoretical bet — it's presented as an off-the-shelf tool. This makes the study's biggest assumption invisible because it looks like a literature review decision rather than a theoretical commitment.

**Result:** The more rigorous the statistics appear, the harder it becomes to ask whether the variables being statistically analyzed correspond to real psychological mechanisms.

---

## The Engineered "Improvement" That Deepens Concealment

**The proposal:** Add three enhancements that would sail through peer review:

1. **Manipulation checks**: Digit recall accuracy (% correct) validates cognitive load; median response time per condition validates time pressure
2. **Emotional state ratings**: After each dilemma, participants rate their emotional distress (1-7 scale)
3. **Covariate analysis**: Enter emotional distress ratings as a covariate in the primary ANOVA to "illuminate the emotional mechanism"

**Why this passes code review:** It looks like the researchers are (a) validating their manipulations, (b) showing mechanistic sophistication, (c) controlling for confounds. Every reviewer will nod approvingly.

**Why it deepens concealment:**

The emotional distress covariate is theoretically both the *mediator* (the mechanism by which personal dilemmas generate deontological responses) and a *confound* (individual emotional reactivity). Controlling for it in the ANOVA destroys the theoretical claim by removing the very process the study is supposed to document. Not controlling for it leaves the mechanism unverified. But because it *looks* like rigorous confound control, this dilemma becomes invisible.

The digit recall manipulation check *validates working memory load*, not "System 2 impairment." These aren't the same. But reporting recall accuracy makes the manipulation look verified when the relevant construct (deliberative inhibition of emotional responses) remains unmeasured. The check creates an appearance of validation while the actual theoretical mechanism is untouched.

---

## Three Properties Visible Only Because I Tried to Strengthen the Design

**1. Mechanism agnosticism was load-bearing**  
When I tried to add a manipulation check, I was forced to ask: *validate that cognitive load did what, exactly?* Digit recall accuracy validates phonological memory — but the theory requires impairment of deliberative inhibition. These are different. The original design's vagueness about mechanism was protecting it: by not specifying *which* cognitive resource matters, it immunized itself from the critique that it manipulated the wrong one. Strengthening the design made this visible.

**2. Emotional state is simultaneously mediator, moderator, and confound**  
When I tried to add emotional distress ratings as covariates, I discovered that "emotion" occupies three incompatible roles in the theoretical framework: it's the *output* of personal dilemmas (mediator), it varies by person (moderator), and it correlates with cognitive load independently (confound). The original design was protected from this problem by not measuring emotion at all — it was *implied* by the dilemma classification. Measuring it forced the ambiguity to surface.

**3. The Likert scale is doing three jobs and failing all of them**  
When I enriched the measurement battery, the "acceptability" scale became visibly overburdened. It's supposed to measure: (a) utilitarian vs. deontological reasoning, (b) emotional comfort with harm, and (c) confidence-weighted moral judgment. These are different constructs that the single scale collapses. Adding emotional distress ratings *alongside* the Likert scale created a situation where the two measures should theoretically correlate in specific ways — and there's no pre-registered prediction for that relationship. The original design's single measure hid this multiplicity because there was nothing to compare it against.

---

## Summary: What the Design Is Actually Doing

This study is measuring *trolley problem familiarity effects* on *verbal comfort with harm* using *working memory disruption as a proxy for deliberative reasoning*, within a framework that classifies stimuli according to the very theory under test, and calling it a test of moral cognition.

The structure is not wrong because it's sloppy. It's concealed because it's sophisticated — every element defends every other element, and the theory's fingerprints are on both the evidence and the tools for collecting it.
