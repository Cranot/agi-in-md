# Structural Diagnosis: Level 7 Diagnostic Gap Analysis

## The Initial Claim (Specific, Falsifiable)

**The deepest structural problem is that the power analysis is computed for the wrong effect.** The study's theoretical contribution depends entirely on the three-way interaction (load × time pressure × dilemma type), but the power analysis targets a between-subjects main effect at d=0.5 with 30 per cell. Three-way interactions in mixed designs typically require 4–8× the sample needed for main effects. This study is powered for a test it doesn't need and underpowered for the test it does, making the critical hypothesis test structurally decorative.

---

## The Dialectic

**Expert A (Defender):** Correct. The focused contrast—where high-load/time-pressure specifically shifts personal but not impersonal dilemmas—is a cell-mean difference buried inside a three-way interaction. At N=30/cell, the study has roughly 20–30% power for this effect. The design cannot test its own hypothesis.

**Expert B (Attacker):** Overstated. The repeated-measures component (dilemma type, within-subject) dramatically increases power for the interaction involving that factor. The authors could also argue d=0.5 targets their critical planned contrast, not the omnibus main effect. With 12 observations per person (6 per dilemma type), the within-subject variance reduction is substantial. The power situation is defensible if not pristine.

**Expert C (Probing what both assume):** Both of you treat d=0.5 as an anchor worth arguing about. Where does it come from? Published effects in the cognitive-load-and-moral-judgment literature are almost certainly inflated by publication bias, selective reporting, and the garden of forking paths. The real issue isn't "is 30/cell enough for d=0.5"—it's that d=0.5 is probably a ghost number reflecting literature-wide bias. You can't be correctly or incorrectly powered for an effect size that doesn't correspond to reality.

## The Transformed Claim

The deepest structural problem is not miscalculated power but **unknowable** power. The entire quantitative scaffolding—power analysis, ANOVA, Bonferroni correction—creates an apparatus of methodological rigor anchored to an effect-size estimate whose provenance is unexamined and likely reflects inflated published effects. The study cannot even accurately assess its own inadequacy. The formalism produces the sensation of rigor while the foundation is epistemically hollow.

---

## The Gap and What It Reveals

| | Original Claim | Transformed Claim |
|---|---|---|
| **Nature of problem** | Calculable mismatch | Incalculable uncertainty |
| **Fix implied** | Increase N to ~400+ | No simple fix; requires different epistemology |
| **Locus of error** | Researcher's arithmetic | The field's evidentiary infrastructure |

The gap moves from *"you did the math wrong"* to *"the math is grounded in nothing."*

### The Concealment Mechanism: **Methodological Formalism as Epistemic Camouflage**

The design presents a complete, interlocking apparatus: IRB approval, power analysis, pre-specified ANOVA, exclusion criteria, Bonferroni correction, covariates. Each element is individually defensible. Their **collective** function is to answer *"how will you analyze this?"* so comprehensively that a deeper question becomes invisible: *"does your prediction follow from any coherent theory?"*

The formalism performs the rituals of rigor, and the performance is so thorough that it crowds out scrutiny of the theoretical structure underneath.

---

## Applying the Concealment Mechanism: What Is STILL Hidden

Now I apply the identified mechanism—look for where methodological formalism prevents examination of theoretical coherence—and ask what the entire dialectic (including the three experts' debate about power, effect sizes, and estimation) failed to surface.

### The prediction is backwards.

The study implicitly operates within Greene's dual-process framework. This is evident from the design's DNA: the personal/impersonal dilemma distinction **is** Greene's taxonomy; the utilitarian/deontological response framing **is** Greene's dependent variable; the cognitive-load manipulation **is** the standard tool for testing dual-process accounts.

Under Greene's model:
- **Personal moral dilemmas** trigger automatic emotional responses (System 1) → deontological judgments (*"don't push the man"*)
- **Utilitarian responses** to personal dilemmas require **cognitively effortful override** of these emotional responses (System 2)
- Therefore: **cognitive load should DECREASE utilitarian responding**, because it depletes the very resource needed to override emotional intuition

**The study predicts the opposite**: that high load + time pressure *increases* utilitarian responses to personal dilemmas.

This prediction is **internally incoherent with the theoretical framework the design embodies**. For the prediction to hold, one of these must be true:

1. **Emotional processing itself requires working memory** to translate into judgment (an "affect-as-information" model where load blocks emotional input from reaching the response). But this contradicts the automatic/effortless nature of System 1 in the dual-process account the design assumes.

2. **The study is testing a different theory entirely**—perhaps that extreme constraint produces "cold" default responding. But this theory is never named, never specified, and generates different auxiliary predictions the design doesn't test.

3. **The researchers confused the direction of the standard prediction.** The most parsimonious explanation.

### Why the dialectic missed this

All three experts—and my original claim—focused on the **statistical** architecture: power, effect sizes, estimation. This is exactly how methodological formalism conceals theoretical incoherence. The elaborate analytical apparatus draws critical attention toward quantitative adequacy and away from the qualitative question of whether the hypothesis is derivable from any consistent model of the phenomenon.

---

## Summary of the Full Diagnostic

| Layer | Problem | How concealed |
|---|---|---|
| **Surface** | Possible underpowering for interactions | Partially visible; debatable |
| **Middle** | Power analysis anchored to inflated effect sizes | Hidden by the formalism of having *done* a power analysis |
| **Deep (still hidden)** | The prediction may contradict its own theoretical framework | Hidden by the methodological apparatus redirecting all scrutiny to analytical procedures |

The study's most serious structural flaw is not statistical—it is that **the hypothesis may not follow from any coherent theory**, and the design is constructed so that this incoherence is the last thing anyone examines. The concealment mechanism is the completeness of the methodological formalism itself: a study that looks rigorous in every procedural detail while potentially predicting the opposite of what its own theoretical commitments entail.
