# Structure First: Experimental Design Analysis

## THE CLAIM (Falsifiable Hypothesis)

**The study's deepest structural problem is a theory-prediction inversion**: The stated prediction—that high cognitive load + time pressure *increases* utilitarian responses to personal dilemmas—contradicts what the dual-process framework motivating this design actually predicts. This creates a specific failure mode: any pattern of results will be uninterpretable with respect to the theoretical claims being tested. If results confirm the prediction, there is no coherent mechanism to explain why. If they disconfirm it, the study has accidentally replicated what dominant theory expects, but this registers as failure rather than confirmation. The design cannot win epistemically.

This is falsifiable: **if a study can produce results that confirm its theoretical framework regardless of whether the prediction is confirmed or disconfirmed, the prediction was structurally inverted from the start.**

---

## EXPERT 1 — DEFENDER (Cognitive Moral Psychology)

The inversion is real and consequential. Greene's dual-process account—the clear theoretical ancestor of this design—predicts that cognitive load disrupts deliberative System 2 processing while leaving emotional System 1 responses intact. Personal dilemmas (footbridge, not trolley) trigger automatic deontological emotional reactions. Load should *suppress* utilitarian responses by preventing the effortful override of those emotions. The study predicts the opposite.

There is no specified mechanism offered for why load + time pressure would *release* utilitarian responding. The closest candidates in the literature would require a completely different theoretical commitment: perhaps that utilitarian responses are System 1 defaults that get suppressed by emotional-regulatory *deliberation*—roughly Cushman's (2013) model-free/model-based framework, or Fischer & Engelmann's suppression account. But this framework would demand different dilemma construction, different manipulation checks, and different analysis entirely. The study sits between two incompatible theories without committing to either.

Crucially: this isn't a minor specification error. The prediction is the hypothesis. If the prediction is theoretically incoherent, the study has no falsifiable theoretical claim left—only a behavioral pattern claim about ratings, which proves nothing about cognitive processes.

---

## EXPERT 2 — ATTACKER (Research Methods)

The theory-inversion claim is overstated on three grounds.

**First**, the empirical literature is genuinely inconsistent. Several replicated studies (Suter & Hertwig, 2011; Trémolière & Bonnefon, 2014) *do* find increased utilitarian responding under various load conditions. Calling a prediction a "contradiction" of dual-process theory requires a settled theory, which we don't have. The field is contested precisely because the prediction space is not well-constrained.

**Second**, the prediction is mechanistically underspecified but not incoherent. If emotional responses to personal dilemmas are themselves cognitively demanding (requiring attention to facial features, imagined suffering, social calculus), then taxing cognition might disrupt the *emotional* process more than the utilitarian calculation. This is testable. The study doesn't commit to this mechanism, but the attacker's point is that theory-inversion requires a definitive theory—which doesn't exist here.

**Third, and most importantly**, the defender has identified the wrong depth. The theory-inversion problem is at the hypothesis level. The deeper problem is at the *design* level: acceptability ratings on pre-classified dilemmas cannot distinguish between utilitarian outcomes reached by utilitarian reasoning processes and identical ratings reached by deference, rule-following, emotional regulation failure, or demand compliance. This measurement-process gap is not fixable by choosing a better theoretical prediction. It's structural to the measurement architecture itself.

---

## EXPERT 3 — PROBER (Philosophy of Science)

Both arguments share assumptions that need examination.

**What the defender takes for granted**: That dual-process theory is the correct evaluative framework, and that "personal dilemma" reliably operationalizes "emotionally engaging stimulus." Kahane et al. (2018) and Bostyn et al. (2018) show that personal/impersonal dilemma categories co-vary with at least seven other psychologically relevant variables—physical proximity, causation type, identifiability, agent instrumentality. The category is doing too much work. Defenders of the theory-inversion claim inherit the same category problem.

**What the attacker takes for granted**: That the study can generate *any* theoretically meaningful inference about cognitive processes from behavioral ratings. The attacker says the DV confound is deeper than theory-inversion—but this concedes too much. If ratings cannot distinguish reasoning processes, then the attacker's "this is still testable" defense collapses. What remains testable is only the rating pattern, not the cognitive claim.

**What both take for granted**: That "utilitarian response" is a property of the stimulus (the dilemma) rather than a property of the reasoner's process. The study treats personal dilemmas as having a correct utilitarian answer that participants either endorse or don't. But a participant may rate "pushing someone off a bridge" as acceptable while reasoning deontologically ("the rules say minimize aggregate harm, so accept it") or as unacceptable while reasoning consequentially ("outcomes are too uncertain"). The Likert scale flattens this completely.

**The deeper assumption being smuggled in**: That there is a clean mapping between experimentally observable behavioral outputs and the latent cognitive processes the theory is about. Neither the defender nor the attacker questions whether this mapping is achievable in principle with this design. Both debate which theoretical framework best explains ratings—but ratings may be theoretically opaque regardless of framework.

---

## SYNTHESIS: Did the Argument Falsify, Strengthen, or Transform the Claim?

**The argument transformed the claim—and in doing so, strengthened it at a deeper level.**

### What changed

The initial claim was about theory-prediction inversion: a testable and real problem. The argument revealed that theory-inversion is a *symptom* of a more fundamental structure, not the disease itself.

The defender showed the inversion is real. The attacker showed it's less unique than it seems (the literature is messy) while identifying a deeper flaw. The prober showed that both were arguing at the wrong level—the structural problem precedes the theoretical one.

### The transformed claim

**The study's deepest problem is not that the prediction contradicts dual-process theory. It's that the study cannot, in principle, generate evidence about cognitive processes using behavioral ratings on pre-classified dilemmas—regardless of which theory is assumed and regardless of whether results confirm or disconfirm the prediction.**

The design is falsifiable as a test of *rating patterns under experimental conditions*. It is structurally unfalsifiable as a test of *dual-process claims about moral reasoning*. These are two different studies, and the researchers have written the latter while building only the former.

### What we know now that we couldn't before the argument

**1. The theory-inversion and DV-confound are the same problem at different levels of resolution.** Theory is confused because measurement was never adequate to distinguish theories. If you can't measure process, you can't settle theoretical disputes. These aren't independent flaws—one generates the other.

**2. The falsifiability problem has a specific structure.** The study IS falsifiable (significance or not) but results can't be interpreted as process claims. This is the difference between "falsifiable" and "interpretable in the domain of interest." Most critiques of studies conflate these. The problem here is interpretability, not falsifiability—and that's a harder problem.

**3. The personal/impersonal dilemma variable is doing structural work it cannot support.** This variable is simultaneously: (a) the theoretical construct (emotionally engaging vs. not), (b) the repeated-measures factor in the ANOVA, and (c) the primary moderator of the predicted interaction. One variable cannot validly serve all three roles. Controlling for it statistically doesn't resolve the conceptual confound.

**4. No statistical sophistication fixes this.** Bonferroni corrections, response time covariates, confidence ratings—these address precision and multiple comparison problems. None of them address the measurement-process gap. The fix requires either process-tracing methods (think-aloud protocols, decomposed utility measurement, drift-diffusion modeling of response time distributions) or an explicit theoretical reformulation that commits to what acceptability ratings actually measure.

### The remaining question the argument didn't resolve

If the mapping between behavioral ratings and cognitive processes is as broken as the prober suggests, is the personal/impersonal dilemma paradigm salvageable at all, or has the entire methodology been asking the wrong question for thirty years? That question wasn't answered—but now it's the right question to ask.
